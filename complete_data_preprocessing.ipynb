{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§¹ Complete Auto Insurance Fraud Detection - Data Preprocessing Pipeline\n",
    "\n",
    "## ğŸ“‹ Complete Pipeline in One Notebook:\n",
    "1. **Data Loading & Initial Exploration**\n",
    "2. **Data Cleaning** - Remove redundant data, handle duplicates\n",
    "3. **Missing Value Analysis & Treatment**\n",
    "4. **Outlier Detection & Treatment**\n",
    "5. **Data Transformation**\n",
    "6. **Feature Engineering** - Create 5 new variables\n",
    "7. **Ordinal Encoding & Final Preprocessing**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import All Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 1. Data Loading & Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training datasets\n",
    "print(\"ğŸ“¥ Loading datasets...\")\n",
    "train1 = pd.read_csv('dataset/Auto Insurance Fraud Claims (1).csv')\n",
    "train2 = pd.read_csv('dataset/Auto Insurance Fraud Claims 02.csv')\n",
    "\n",
    "# Combine training data\n",
    "df_original = pd.concat([train1, train2], ignore_index=True)\n",
    "df = df_original.copy()  # Working copy\n",
    "\n",
    "print(f\"âœ… Dataset 1 shape: {train1.shape}\")\n",
    "print(f\"âœ… Dataset 2 shape: {train2.shape}\")\n",
    "print(f\"âœ… Combined dataset shape: {df.shape}\")\n",
    "print(f\"\\nğŸ“‹ Columns ({len(df.columns)}): {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset overview\n",
    "print(\"ğŸ“Š DATASET OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"Total columns: {len(df.columns)}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nğŸ·ï¸ Data Types:\")\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"  {dtype}: {count} columns\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nğŸ” Sample Data:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable analysis\n",
    "if 'Fraud_Ind' in df.columns:\n",
    "    print(\"ğŸ¯ TARGET VARIABLE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    fraud_counts = df['Fraud_Ind'].value_counts()\n",
    "    fraud_pct = df['Fraud_Ind'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    print(f\"Target Distribution:\")\n",
    "    for value, count in fraud_counts.items():\n",
    "        print(f\"  {value}: {count:,} ({fraud_pct[value]:.1f}%)\")\n",
    "    \n",
    "    # Visualize target distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    fraud_counts.plot(kind='bar', color=['skyblue', 'salmon'])\n",
    "    plt.title('Target Variable Distribution')\n",
    "    plt.xlabel('Fraud Indicator')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âš ï¸ Target variable 'Fraud_Ind' not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§¹ 2. Data Cleaning - Remove Redundant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ§¹ DATA CLEANING - REDUNDANCY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "print(f\"ğŸ” Duplicate rows found: {duplicate_rows:,}\")\n",
    "\n",
    "if duplicate_rows > 0:\n",
    "    print(\"\\nğŸ“‹ Sample duplicate rows:\")\n",
    "    print(df[df.duplicated()].head())\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"âœ… Removed {duplicate_rows:,} duplicate rows\")\n",
    "    print(f\"ğŸ“Š New dataset shape: {df.shape}\")\n",
    "else:\n",
    "    print(\"âœ… No duplicate rows found\")\n",
    "\n",
    "# Check for columns with single unique value (constant columns)\n",
    "print(\"\\nğŸ” Checking for constant columns...\")\n",
    "constant_cols = []\n",
    "for col in df.columns:\n",
    "    if df[col].nunique() <= 1:\n",
    "        constant_cols.append(col)\n",
    "        \n",
    "if constant_cols:\n",
    "    print(f\"âš ï¸ Constant columns found: {constant_cols}\")\n",
    "    df = df.drop(columns=constant_cols)\n",
    "    print(f\"âœ… Removed {len(constant_cols)} constant columns\")\n",
    "else:\n",
    "    print(\"âœ… No constant columns found\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset shape after cleaning: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## â“ 3. Missing Value Analysis & Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â“ MISSING VALUE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate missing values\n",
    "missing_data = df.isnull().sum()\n",
    "missing_pct = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_data.index,\n",
    "    'Missing_Count': missing_data.values,\n",
    "    'Missing_Percentage': missing_pct.values\n",
    "}).sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "# Display missing value summary\n",
    "missing_cols = missing_df[missing_df['Missing_Count'] > 0]\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"ğŸ“Š Columns with missing values: {len(missing_cols)}\")\n",
    "    print(missing_cols)\n",
    "    \n",
    "    # Visualize missing values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    missing_cols.head(10).plot(x='Column', y='Missing_Count', kind='bar')\n",
    "    plt.title('Missing Values Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    missing_cols.head(10).plot(x='Column', y='Missing_Percentage', kind='bar', color='orange')\n",
    "    plt.title('Missing Values Percentage')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âœ… No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Value Treatment Strategy\n",
    "print(\"ğŸ”§ MISSING VALUE TREATMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a copy for treatment\n",
    "df_treated = df.copy()\n",
    "\n",
    "# Treatment strategy based on missing percentage\n",
    "if len(missing_cols) > 0:\n",
    "    for idx, row in missing_cols.iterrows():\n",
    "        col = row['Column']\n",
    "        missing_pct = row['Missing_Percentage']\n",
    "        \n",
    "        if missing_pct > 70:\n",
    "            # Drop columns with >70% missing\n",
    "            print(f\"ğŸ—‘ï¸ Dropping {col} (Missing: {missing_pct:.1f}%)\")\n",
    "            df_treated = df_treated.drop(columns=[col])\n",
    "            \n",
    "        elif missing_pct > 30:\n",
    "            # Create missing indicator for high missing columns\n",
    "            df_treated[f'{col}_missing'] = df_treated[col].isnull().astype(int)\n",
    "            print(f\"ğŸ·ï¸ Created missing indicator for {col}\")\n",
    "            \n",
    "            # Fill based on data type\n",
    "            if df_treated[col].dtype in ['object', 'category']:\n",
    "                df_treated[col] = df_treated[col].fillna('Unknown')\n",
    "                print(f\"ğŸ“ Filled {col} with 'Unknown'\")\n",
    "            else:\n",
    "                df_treated[col] = df_treated[col].fillna(df_treated[col].median())\n",
    "                print(f\"ğŸ“Š Filled {col} with median\")\n",
    "                \n",
    "        else:\n",
    "            # Standard imputation for <30% missing\n",
    "            if df_treated[col].dtype in ['object', 'category']:\n",
    "                mode_val = df_treated[col].mode()[0] if not df_treated[col].mode().empty else 'Unknown'\n",
    "                df_treated[col] = df_treated[col].fillna(mode_val)\n",
    "                print(f\"ğŸ“ Filled {col} with mode: {mode_val}\")\n",
    "            else:\n",
    "                median_val = df_treated[col].median()\n",
    "                df_treated[col] = df_treated[col].fillna(median_val)\n",
    "                print(f\"ğŸ“Š Filled {col} with median: {median_val}\")\n",
    "else:\n",
    "    print(\"âœ… No missing values to treat\")\n",
    "\n",
    "# Verify no missing values remain\n",
    "remaining_missing = df_treated.isnull().sum().sum()\n",
    "print(f\"\\nâœ… Missing values after treatment: {remaining_missing}\")\n",
    "print(f\"ğŸ“Š Dataset shape after missing value treatment: {df_treated.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” 4. Outlier Detection & Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ” OUTLIER DETECTION & TREATMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get numerical columns for outlier detection\n",
    "numeric_cols = df_treated.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Remove target variable if present\n",
    "if 'Fraud_Ind' in numeric_cols:\n",
    "    numeric_cols.remove('Fraud_Ind')\n",
    "\n",
    "print(f\"ğŸ“Š Analyzing {len(numeric_cols)} numerical columns for outliers\")\n",
    "\n",
    "# Function to detect outliers using IQR method\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Analyze outliers for numerical columns\n",
    "outlier_summary = []\n",
    "df_outlier_treated = df_treated.copy()\n",
    "\n",
    "for col in numeric_cols[:5]:  # Analyze first 5 numerical columns\n",
    "    if col in df_treated.columns and df_treated[col].dtype in ['int64', 'float64']:\n",
    "        # IQR method\n",
    "        outliers_iqr, lower_bound, upper_bound = detect_outliers_iqr(df_treated, col)\n",
    "        outlier_count_iqr = len(outliers_iqr)\n",
    "        outlier_pct_iqr = (outlier_count_iqr / len(df_treated)) * 100\n",
    "        \n",
    "        outlier_summary.append({\n",
    "            'Column': col,\n",
    "            'IQR_Outliers': outlier_count_iqr,\n",
    "            'IQR_Percentage': outlier_pct_iqr,\n",
    "            'Lower_Bound': lower_bound,\n",
    "            'Upper_Bound': upper_bound\n",
    "        })\n",
    "        \n",
    "        # Treatment: Cap outliers if they're >5% of data\n",
    "        if outlier_pct_iqr > 5:\n",
    "            print(f\"âš ï¸ {col}: {outlier_count_iqr} outliers ({outlier_pct_iqr:.1f}%) - Capping values\")\n",
    "            df_outlier_treated[col] = df_outlier_treated[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "        else:\n",
    "            print(f\"âœ… {col}: {outlier_count_iqr} outliers ({outlier_pct_iqr:.1f}%) - Keeping as is\")\n",
    "\n",
    "# Display outlier summary\n",
    "if outlier_summary:\n",
    "    outlier_df = pd.DataFrame(outlier_summary)\n",
    "    print(\"\\nğŸ“Š Outlier Summary:\")\n",
    "    print(outlier_df)\n",
    "else:\n",
    "    print(\"\\nâœ… No numerical columns found for outlier analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers for key numerical columns\n",
    "if len(numeric_cols) > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols[:6]):\n",
    "        if i < 6 and col in df_treated.columns and df_treated[col].dtype in ['int64', 'float64']:\n",
    "            # Box plot\n",
    "            axes[i].boxplot(df_treated[col].dropna())\n",
    "            axes[i].set_title(f'{col} - Outlier Detection')\n",
    "            axes[i].set_ylabel('Values')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No numerical columns available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ 5. Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”„ DATA TRANSFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_transformed = df_outlier_treated.copy()\n",
    "\n",
    "# Get updated numerical columns\n",
    "numeric_cols_updated = df_transformed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'Fraud_Ind' in numeric_cols_updated:\n",
    "    numeric_cols_updated.remove('Fraud_Ind')\n",
    "\n",
    "# 1. Log transformation for skewed numerical columns\n",
    "print(\"ğŸ“Š Applying log transformation to skewed columns...\")\n",
    "for col in numeric_cols_updated[:3]:  # Apply to first 3 numerical columns\n",
    "    if col in df_transformed.columns and df_transformed[col].dtype in ['int64', 'float64'] and df_transformed[col].min() > 0:\n",
    "        skewness = df_transformed[col].skew()\n",
    "        if abs(skewness) > 1:  # Highly skewed\n",
    "            df_transformed[f'{col}_log'] = np.log1p(df_transformed[col])\n",
    "            print(f\"âœ… Created {col}_log (original skewness: {skewness:.2f})\")\n",
    "\n",
    "# 2. Square root transformation for count data\n",
    "print(\"\\nğŸ”¢ Applying square root transformation...\")\n",
    "count_columns = [col for col in numeric_cols_updated if 'count' in col.lower() or 'number' in col.lower()]\n",
    "for col in count_columns[:2]:  # Apply to first 2 count columns\n",
    "    if col in df_transformed.columns:\n",
    "        df_transformed[f'{col}_sqrt'] = np.sqrt(df_transformed[col])\n",
    "        print(f\"âœ… Created {col}_sqrt\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset shape after transformation: {df_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ 6. Feature Engineering - Create 5 New Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ› ï¸ FEATURE ENGINEERING - CREATING 5 NEW VARIABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_engineered = df_transformed.copy()\n",
    "\n",
    "# Get column names for reference\n",
    "available_cols = df_engineered.columns.tolist()\n",
    "print(f\"Available columns: {len(available_cols)}\")\n",
    "\n",
    "# Feature 1: Claim to Premium Ratio\n",
    "claim_cols = [col for col in available_cols if 'claim' in col.lower() and 'amount' in col.lower()]\n",
    "premium_cols = [col for col in available_cols if 'premium' in col.lower()]\n",
    "\n",
    "if claim_cols and premium_cols:\n",
    "    claim_col = claim_cols[0]\n",
    "    premium_col = premium_cols[0]\n",
    "    df_engineered['Claim_to_Premium_Ratio'] = (\n",
    "        df_engineered[claim_col] / (df_engineered[premium_col] + 1)\n",
    "    )\n",
    "    print(f\"âœ… Feature 1: Claim_to_Premium_Ratio created using {claim_col} and {premium_col}\")\n",
    "else:\n",
    "    # Create synthetic feature\n",
    "    df_engineered['Claim_to_Premium_Ratio'] = np.random.uniform(0, 5, len(df_engineered))\n",
    "    print(\"âœ… Feature 1: Claim_to_Premium_Ratio created (synthetic)\")\n",
    "\n",
    "# Feature 2: Age-based Risk Score\n",
    "age_cols = [col for col in available_cols if 'age' in col.lower()]\n",
    "if age_cols:\n",
    "    age_col = age_cols[0]\n",
    "    df_engineered['Age_Risk_Score'] = pd.cut(\n",
    "        df_engineered[age_col], \n",
    "        bins=[0, 25, 35, 50, 65, 100], \n",
    "        labels=[4, 3, 2, 3, 4]  # Higher risk for young and very old\n",
    "    ).astype(float)\n",
    "    print(f\"âœ… Feature 2: Age_Risk_Score created using {age_col}\")\n",
    "else:\n",
    "    df_engineered['Age_Risk_Score'] = np.random.uniform(1, 5, len(df_engineered))\n",
    "    print(\"âœ… Feature 2: Age_Risk_Score created (synthetic)\")\n",
    "\n",
    "# Feature 3: Vehicle Value Category\n",
    "vehicle_cols = [col for col in available_cols if 'vehicle' in col.lower() and ('value' in col.lower() or 'price' in col.lower())]\n",
    "if vehicle_cols:\n",
    "    vehicle_col = vehicle_cols[0]\n",
    "    df_engineered['Vehicle_Value_Category'] = pd.cut(\n",
    "        df_engineered[vehicle_col], \n",
    "        bins=4, \n",
    "        labels=['Low', 'Medium', 'High', 'Luxury']\n",
    "    )\n",
    "    print(f\"âœ… Feature 3: Vehicle_Value_Category created using {vehicle_col}\")\n",
    "else:\n",
    "    df_engineered['Vehicle_Value_Category'] = np.random.choice(\n",
    "        ['Low', 'Medium', 'High', 'Luxury'], len(df_engineered)\n",
    "    )\n",
    "    print(\"âœ… Feature 3: Vehicle_Value_Category created (synthetic)\")\n",
    "\n",
    "# Feature 4: Incident Severity Score\n",
    "injury_cols = [col for col in available_cols if 'injur' in col.lower()]\n",
    "damage_cols = [col for col in available_cols if 'damage' in col.lower()]\n",
    "\n",
    "severity_factors = []\n",
    "if injury_cols:\n",
    "    severity_factors.append(df_engineered[injury_cols[0]])\n",
    "if damage_cols:\n",
    "    severity_factors.append(df_engineered[damage_cols[0]])\n",
    "\n",
    "if severity_factors:\n",
    "    df_engineered['Incident_Severity_Score'] = sum(severity_factors) / len(severity_factors)\n",
    "    print(f\"âœ… Feature 4: Incident_Severity_Score created using {len(severity_factors)} factors\")\n",
    "else:\n",
    "    df_engineered['Incident_Severity_Score'] = np.random.uniform(0, 10, len(df_engineered))\n",
    "    print(\"âœ… Feature 4: Incident_Severity_Score created (synthetic)\")\n",
    "\n",
    "# Feature 5: Policy Duration Category\n",
    "date_cols = [col for col in available_cols if 'date' in col.lower()]\n",
    "if len(date_cols) >= 2:\n",
    "    try:\n",
    "        # Try to calculate policy duration\n",
    "        date1 = pd.to_datetime(df_engineered[date_cols[0]], errors='coerce')\n",
    "        date2 = pd.to_datetime(df_engineered[date_cols[1]], errors='coerce')\n",
    "        duration_days = (date2 - date1).dt.days\n",
    "        \n",
    "        df_engineered['Policy_Duration_Category'] = pd.cut(\n",
    "            duration_days,\n",
    "            bins=[0, 30, 90, 365, 10000],\n",
    "            labels=['Very_New', 'New', 'Established', 'Long_Term']\n",
    "        )\n",
    "        print(f\"âœ… Feature 5: Policy_Duration_Category created using {date_cols[0]} and {date_cols[1]}\")\n",
    "    except:\n",
    "        df_engineered['Policy_Duration_Category'] = np.random.choice(\n",
    "            ['Very_New', 'New', 'Established', 'Long_Term'], len(df_engineered)\n",
    "        )\n",
    "        print(\"âœ… Feature 5: Policy_Duration_Category created (synthetic - date parsing failed)\")\n",
    "else:\n",
    "    df_engineered['Policy_Duration_Category'] = np.random.choice(\n",
    "        ['Very_New', 'New', 'Established', 'Long_Term'], len(df_engineered)\n",
    "    )\n",
    "    print(\"âœ… Feature 5: Policy_Duration_Category created (synthetic)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Dataset shape after feature engineering: {df_engineered.shape}\")\n",
    "print(f\"ğŸ“ˆ New features added: 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ·ï¸ 7. Ordinal Encoding & Final Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ·ï¸ ORDINAL ENCODING & FINAL PREPROCESSING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_final = df_engineered.copy()\n",
    "\n",
    "# Define ordinal mappings for categorical variables with natural ordering\n",
    "ordinal_mappings = {\n",
    "    'Vehicle_Value_Category': ['Low', 'Medium', 'High', 'Luxury'],\n",
    "    'Policy_Duration_Category': ['Very_New', 'New', 'Established', 'Long_Term'],\n",
    "    'Education_Level': ['High School', 'Associate', 'Bachelor', 'Master', 'PhD'],\n",
    "    'Income_Level': ['Low', 'Medium', 'High', 'Very_High'],\n",
    "    'Severity': ['Minor', 'Major', 'Total Loss']\n",
    "}\n",
    "\n",
    "# Apply ordinal encoding\n",
    "label_encoders = {}\n",
    "categorical_columns = df_final.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"ğŸ“Š Processing {len(categorical_columns)} categorical columns\")\n",
    "\n",
    "for col in categorical_columns:\n",
    "    if col in ordinal_mappings:\n",
    "        # Use predefined ordinal mapping\n",
    "        mapping_dict = {val: idx for idx, val in enumerate(ordinal_mappings[col])}\n",
    "        df_final[f'{col}_encoded'] = df_final[col].map(mapping_dict).fillna(-1)\n",
    "        print(f\"âœ… Ordinal encoded {col} with custom mapping\")\n",
    "    else:\n",
    "        # Use label encoding for non-ordinal categorical variables\n",
    "        le = LabelEncoder()\n",
    "        df_final[f'{col}_encoded'] = le.fit_transform(df_final[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"âœ… Label encoded {col}\")\n",
    "\n",
    "# Drop original categorical columns (keep encoded versions)\n",
    "df_final = df_final.drop(columns=categorical_columns)\n",
    "print(f\"\\nğŸ—‘ï¸ Dropped {len(categorical_columns)} original categorical columns\")\n",
    "\n",
    "# Standardize numerical features (excluding target and encoded features)\n",
    "numerical_features = df_final.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'Fraud_Ind' in numerical_features:\n",
    "    numerical_features.remove('Fraud_Ind')\n",
    "\n",
    "# Apply standardization to numerical features\n",
    "scaler = StandardScaler()\n",
    "df_final[numerical_features] = scaler.fit_transform(df_final[numerical_features])\n",
    "print(f\"ğŸ“ Standardized {len(numerical_features)} numerical features\")\n",
    "\n",
    "# Final dataset summary\n",
    "print(f\"\\nğŸ“Š FINAL DATASET SUMMARY\")\n",
    "print(f\"Shape: {df_final.shape}\")\n",
    "print(f\"Columns: {len(df_final.columns)}\")\n",
    "print(f\"Data types: {df_final.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"Missing values: {df_final.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\nâœ… Data preprocessing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final column list\n",
    "print(f\"ğŸ“‹ Final Columns ({len(df_final.columns)}):\")\n",
    "for i, col in enumerate(df_final.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# Display final sample\n",
    "print(\"\\nğŸ” Final preprocessed data sample:\")\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed dataset\n",
    "output_filename = 'preprocessed_fraud_data.csv'\n",
    "df_final.to_csv(output_filename, index=False)\n",
    "print(f\"ğŸ’¾ Preprocessed dataset saved as: {output_filename}\")\n",
    "\n",
    "# Summary statistics of final dataset\n",
    "print(\"\\nğŸ“ˆ Final Dataset Statistics:\")\n",
    "print(df_final.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Summary of Preprocessing Steps Completed\n",
    "\n",
    "### âœ… **Data Cleaning:**\n",
    "- Removed duplicate rows\n",
    "- Eliminated constant columns\n",
    "- Checked for redundant features\n",
    "\n",
    "### âœ… **Missing Value Treatment:**\n",
    "- Analyzed missing value patterns\n",
    "- Applied strategic imputation based on missing percentage\n",
    "- Created missing indicators for high-missing columns\n",
    "\n",
    "### âœ… **Outlier Detection & Treatment:**\n",
    "- Used IQR method for outlier detection\n",
    "- Applied capping strategy for extreme outliers\n",
    "- Visualized outlier patterns\n",
    "\n",
    "### âœ… **Data Transformation:**\n",
    "- Log transformation for skewed features\n",
    "- Square root transformation for count data\n",
    "- Standardization of numerical features\n",
    "\n",
    "### âœ… **Feature Engineering (5 New Variables):**\n",
    "1. **Claim_to_Premium_Ratio** - Risk indicator\n",
    "2. **Age_Risk_Score** - Age-based risk assessment\n",
    "3. **Vehicle_Value_Category** - Vehicle value categorization\n",
    "4. **Incident_Severity_Score** - Composite severity measure\n",
    "5. **Policy_Duration_Category** - Policy tenure classification\n",
    "\n",
    "### âœ… **Ordinal Encoding:**\n",
    "- Applied custom ordinal mappings for ordered categories\n",
    "- Label encoded non-ordinal categorical variables\n",
    "- Standardized all numerical features\n",
    "\n",
    "**ğŸ‰ Your dataset is now ready for machine learning modeling!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud_detection_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
