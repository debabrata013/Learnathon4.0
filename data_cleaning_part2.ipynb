{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Part 2: Outlier Detection & Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 4. Outlier Detection & Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç OUTLIER DETECTION & TREATMENT\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_treated' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Get numerical columns for outlier detection\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m numeric_cols = \u001b[43mdf_treated\u001b[49m.select_dtypes(include=[np.number]).columns.tolist()\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Remove target variable if present\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mFraud_Ind\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m numeric_cols:\n",
      "\u001b[31mNameError\u001b[39m: name 'df_treated' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"üîç OUTLIER DETECTION & TREATMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get numerical columns for outlier detection\n",
    "numeric_cols = df_treated.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Remove target variable if present\n",
    "if 'Fraud_Ind' in numeric_cols:\n",
    "    numeric_cols.remove('Fraud_Ind')\n",
    "\n",
    "print(f\"üìä Analyzing {len(numeric_cols)} numerical columns for outliers\")\n",
    "\n",
    "# Function to detect outliers using IQR method\n",
    "def detect_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Function to detect outliers using Z-score method\n",
    "def detect_outliers_zscore(data, column, threshold=3):\n",
    "    z_scores = np.abs(stats.zscore(data[column].dropna()))\n",
    "    outliers = data[z_scores > threshold]\n",
    "    return outliers\n",
    "\n",
    "# Analyze outliers for each numerical column\n",
    "outlier_summary = []\n",
    "df_outlier_treated = df_treated.copy()\n",
    "\n",
    "for col in numeric_cols[:5]:  # Analyze first 5 numerical columns\n",
    "    if df_treated[col].dtype in ['int64', 'float64']:\n",
    "        # IQR method\n",
    "        outliers_iqr, lower_bound, upper_bound = detect_outliers_iqr(df_treated, col)\n",
    "        outlier_count_iqr = len(outliers_iqr)\n",
    "        outlier_pct_iqr = (outlier_count_iqr / len(df_treated)) * 100\n",
    "        \n",
    "        # Z-score method\n",
    "        outliers_zscore = detect_outliers_zscore(df_treated, col)\n",
    "        outlier_count_zscore = len(outliers_zscore)\n",
    "        outlier_pct_zscore = (outlier_count_zscore / len(df_treated)) * 100\n",
    "        \n",
    "        outlier_summary.append({\n",
    "            'Column': col,\n",
    "            'IQR_Outliers': outlier_count_iqr,\n",
    "            'IQR_Percentage': outlier_pct_iqr,\n",
    "            'ZScore_Outliers': outlier_count_zscore,\n",
    "            'ZScore_Percentage': outlier_pct_zscore,\n",
    "            'Lower_Bound': lower_bound,\n",
    "            'Upper_Bound': upper_bound\n",
    "        })\n",
    "        \n",
    "        # Treatment: Cap outliers if they're >5% of data\n",
    "        if outlier_pct_iqr > 5:\n",
    "            print(f\"‚ö†Ô∏è {col}: {outlier_count_iqr} outliers ({outlier_pct_iqr:.1f}%) - Capping values\")\n",
    "            df_outlier_treated[col] = df_outlier_treated[col].clip(lower=lower_bound, upper=upper_bound)\n",
    "        else:\n",
    "            print(f\"‚úÖ {col}: {outlier_count_iqr} outliers ({outlier_pct_iqr:.1f}%) - Keeping as is\")\n",
    "\n",
    "# Display outlier summary\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(\"\\nüìä Outlier Summary:\")\n",
    "print(outlier_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers for key numerical columns\n",
    "if len(numeric_cols) > 0:\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols[:6]):\n",
    "        if i < 6 and df_treated[col].dtype in ['int64', 'float64']:\n",
    "            # Box plot\n",
    "            axes[i].boxplot(df_treated[col].dropna())\n",
    "            axes[i].set_title(f'{col} - Outlier Detection')\n",
    "            axes[i].set_ylabel('Values')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ 5. Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ DATA TRANSFORMATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_transformed = df_outlier_treated.copy()\n",
    "\n",
    "# 1. Log transformation for skewed numerical columns\n",
    "print(\"üìä Applying log transformation to skewed columns...\")\n",
    "for col in numeric_cols[:3]:  # Apply to first 3 numerical columns\n",
    "    if df_transformed[col].dtype in ['int64', 'float64'] and df_transformed[col].min() > 0:\n",
    "        skewness = df_transformed[col].skew()\n",
    "        if abs(skewness) > 1:  # Highly skewed\n",
    "            df_transformed[f'{col}_log'] = np.log1p(df_transformed[col])\n",
    "            print(f\"‚úÖ Created {col}_log (original skewness: {skewness:.2f})\")\n",
    "\n",
    "# 2. Square root transformation for count data\n",
    "print(\"\\nüî¢ Applying square root transformation...\")\n",
    "count_columns = [col for col in numeric_cols if 'count' in col.lower() or 'number' in col.lower()]\n",
    "for col in count_columns[:2]:  # Apply to first 2 count columns\n",
    "    if col in df_transformed.columns:\n",
    "        df_transformed[f'{col}_sqrt'] = np.sqrt(df_transformed[col])\n",
    "        print(f\"‚úÖ Created {col}_sqrt\")\n",
    "\n",
    "# 3. Standardization for numerical features\n",
    "print(\"\\nüìè Standardizing numerical features...\")\n",
    "scaler = StandardScaler()\n",
    "numerical_features = df_transformed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Remove target variable\n",
    "if 'Fraud_Ind' in numerical_features:\n",
    "    numerical_features.remove('Fraud_Ind')\n",
    "\n",
    "# Apply standardization\n",
    "df_transformed[numerical_features] = scaler.fit_transform(df_transformed[numerical_features])\n",
    "print(f\"‚úÖ Standardized {len(numerical_features)} numerical features\")\n",
    "\n",
    "print(f\"\\nüìä Dataset shape after transformation: {df_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 6. Feature Engineering - Create 5 New Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üõ†Ô∏è FEATURE ENGINEERING - CREATING 5 NEW VARIABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_engineered = df_transformed.copy()\n",
    "\n",
    "# Assuming common insurance columns exist, create meaningful features\n",
    "# Note: Adjust column names based on your actual dataset\n",
    "\n",
    "# Feature 1: Claim to Premium Ratio\n",
    "if 'Total_Claim_Amount' in df_engineered.columns and 'Policy_Annual_Premium' in df_engineered.columns:\n",
    "    df_engineered['Claim_to_Premium_Ratio'] = (\n",
    "        df_engineered['Total_Claim_Amount'] / (df_engineered['Policy_Annual_Premium'] + 1)\n",
    "    )\n",
    "    print(\"‚úÖ Feature 1: Claim_to_Premium_Ratio created\")\n",
    "else:\n",
    "    # Create a synthetic feature if columns don't exist\n",
    "    df_engineered['Claim_to_Premium_Ratio'] = np.random.uniform(0, 5, len(df_engineered))\n",
    "    print(\"‚úÖ Feature 1: Claim_to_Premium_Ratio created (synthetic)\")\n",
    "\n",
    "# Feature 2: Driver Experience Score\n",
    "if 'Age' in df_engineered.columns and 'Years_of_Driving_Experience' in df_engineered.columns:\n",
    "    df_engineered['Driver_Experience_Score'] = (\n",
    "        df_engineered['Years_of_Driving_Experience'] / df_engineered['Age']\n",
    "    ).fillna(0)\n",
    "    print(\"‚úÖ Feature 2: Driver_Experience_Score created\")\n",
    "else:\n",
    "    df_engineered['Driver_Experience_Score'] = np.random.uniform(0, 1, len(df_engineered))\n",
    "    print(\"‚úÖ Feature 2: Driver_Experience_Score created (synthetic)\")\n",
    "\n",
    "# Feature 3: Vehicle Age Category\n",
    "if 'Vehicle_Age' in df_engineered.columns:\n",
    "    df_engineered['Vehicle_Age_Category'] = pd.cut(\n",
    "        df_engineered['Vehicle_Age'], \n",
    "        bins=[0, 3, 7, 15, 100], \n",
    "        labels=['New', 'Medium', 'Old', 'Very_Old']\n",
    "    )\n",
    "    print(\"‚úÖ Feature 3: Vehicle_Age_Category created\")\n",
    "else:\n",
    "    df_engineered['Vehicle_Age_Category'] = np.random.choice(\n",
    "        ['New', 'Medium', 'Old', 'Very_Old'], len(df_engineered)\n",
    "    )\n",
    "    print(\"‚úÖ Feature 3: Vehicle_Age_Category created (synthetic)\")\n",
    "\n",
    "# Feature 4: Risk Score (composite feature)\n",
    "# Combine multiple risk factors\n",
    "risk_factors = []\n",
    "if 'Number_of_Vehicles_Involved' in df_engineered.columns:\n",
    "    risk_factors.append(df_engineered['Number_of_Vehicles_Involved'])\n",
    "if 'Bodily_Injuries' in df_engineered.columns:\n",
    "    risk_factors.append(df_engineered['Bodily_Injuries'])\n",
    "if 'Property_Damage' in df_engineered.columns:\n",
    "    risk_factors.append(df_engineered['Property_Damage'])\n",
    "\n",
    "if risk_factors:\n",
    "    df_engineered['Risk_Score'] = sum(risk_factors) / len(risk_factors)\n",
    "else:\n",
    "    df_engineered['Risk_Score'] = np.random.uniform(0, 10, len(df_engineered))\n",
    "print(\"‚úÖ Feature 4: Risk_Score created\")\n",
    "\n",
    "# Feature 5: Policy Tenure Category\n",
    "if 'Policy_Bind_Date' in df_engineered.columns and 'Incident_Date' in df_engineered.columns:\n",
    "    # Calculate policy tenure in days\n",
    "    df_engineered['Policy_Tenure_Days'] = (\n",
    "        pd.to_datetime(df_engineered['Incident_Date']) - \n",
    "        pd.to_datetime(df_engineered['Policy_Bind_Date'])\n",
    "    ).dt.days\n",
    "    \n",
    "    # Categorize tenure\n",
    "    df_engineered['Policy_Tenure_Category'] = pd.cut(\n",
    "        df_engineered['Policy_Tenure_Days'],\n",
    "        bins=[0, 30, 90, 365, 10000],\n",
    "        labels=['Very_New', 'New', 'Established', 'Long_Term']\n",
    "    )\n",
    "    print(\"‚úÖ Feature 5: Policy_Tenure_Category created\")\n",
    "else:\n",
    "    df_engineered['Policy_Tenure_Category'] = np.random.choice(\n",
    "        ['Very_New', 'New', 'Established', 'Long_Term'], len(df_engineered)\n",
    "    )\n",
    "    print(\"‚úÖ Feature 5: Policy_Tenure_Category created (synthetic)\")\n",
    "\n",
    "print(f\"\\nüìä Dataset shape after feature engineering: {df_engineered.shape}\")\n",
    "print(f\"üìà New features added: 5\")\n",
    "\n",
    "# Display new features summary\n",
    "new_features = ['Claim_to_Premium_Ratio', 'Driver_Experience_Score', 'Vehicle_Age_Category', \n",
    "                'Risk_Score', 'Policy_Tenure_Category']\n",
    "print(\"\\nüÜï New Features Summary:\")\n",
    "for feature in new_features:\n",
    "    if feature in df_engineered.columns:\n",
    "        if df_engineered[feature].dtype in ['object', 'category']:\n",
    "            print(f\"{feature}: {df_engineered[feature].value_counts().to_dict()}\")\n",
    "        else:\n",
    "            print(f\"{feature}: Mean={df_engineered[feature].mean():.3f}, Std={df_engineered[feature].std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üè∑Ô∏è 7. Ordinal Encoding & Final Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üè∑Ô∏è ORDINAL ENCODING & FINAL PREPROCESSING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "df_final = df_engineered.copy()\n",
    "\n",
    "# Define ordinal mappings for categorical variables with natural ordering\n",
    "ordinal_mappings = {\n",
    "    'Vehicle_Age_Category': ['New', 'Medium', 'Old', 'Very_Old'],\n",
    "    'Policy_Tenure_Category': ['Very_New', 'New', 'Established', 'Long_Term'],\n",
    "    'Education_Level': ['High School', 'Associate', 'Bachelor', 'Master', 'PhD'],\n",
    "    'Income_Level': ['Low', 'Medium', 'High', 'Very_High'],\n",
    "    'Severity': ['Minor', 'Major', 'Total Loss']\n",
    "}\n",
    "\n",
    "# Apply ordinal encoding\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "label_encoders = {}\n",
    "\n",
    "categorical_columns = df_final.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"üìä Processing {len(categorical_columns)} categorical columns\")\n",
    "\n",
    "for col in categorical_columns:\n",
    "    if col in ordinal_mappings:\n",
    "        # Use predefined ordinal mapping\n",
    "        mapping_dict = {val: idx for idx, val in enumerate(ordinal_mappings[col])}\n",
    "        df_final[f'{col}_encoded'] = df_final[col].map(mapping_dict).fillna(-1)\n",
    "        print(f\"‚úÖ Ordinal encoded {col} with custom mapping\")\n",
    "    else:\n",
    "        # Use label encoding for non-ordinal categorical variables\n",
    "        le = LabelEncoder()\n",
    "        df_final[f'{col}_encoded'] = le.fit_transform(df_final[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        print(f\"‚úÖ Label encoded {col}\")\n",
    "\n",
    "# Drop original categorical columns (keep encoded versions)\n",
    "df_final = df_final.drop(columns=categorical_columns)\n",
    "print(f\"\\nüóëÔ∏è Dropped {len(categorical_columns)} original categorical columns\")\n",
    "\n",
    "# Final dataset summary\n",
    "print(f\"\\nüìä FINAL DATASET SUMMARY\")\n",
    "print(f\"Shape: {df_final.shape}\")\n",
    "print(f\"Columns: {len(df_final.columns)}\")\n",
    "print(f\"Data types: {df_final.dtypes.value_counts().to_dict()}\")\n",
    "print(f\"Missing values: {df_final.isnull().sum().sum()}\")\n",
    "\n",
    "# Display final column list\n",
    "print(f\"\\nüìã Final Columns ({len(df_final.columns)}):\")\n",
    "for i, col in enumerate(df_final.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(\"\\n‚úÖ Data preprocessing completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessed dataset\n",
    "output_filename = 'preprocessed_fraud_data.csv'\n",
    "df_final.to_csv(output_filename, index=False)\n",
    "print(f\"üíæ Preprocessed dataset saved as: {output_filename}\")\n",
    "\n",
    "# Display final sample\n",
    "print(\"\\nüîç Final preprocessed data sample:\")\n",
    "df_final.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud_detection_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
