{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Insurance Fraud Detection - ML Preprocessing Pipeline\n",
    "\n",
    "## Senior-Level Data Science Approach\n",
    "\n",
    "This notebook implements a comprehensive preprocessing pipeline for auto insurance fraud detection using industry best practices.\n",
    "\n",
    "### Pipeline Overview:\n",
    "1. **Data Loading & Quality Analysis**\n",
    "2. **Missing Value Handling**\n",
    "3. **Duplicate Detection & Removal**\n",
    "4. **Outlier Detection & Treatment**\n",
    "5. **Categorical Feature Encoding**\n",
    "6. **Feature Selection**\n",
    "7. **Feature Normalization**\n",
    "8. **Feature Engineering**\n",
    "9. **Report Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/Users/debabratapattnayak/web-dev/learnathon')\n",
    "\n",
    "# Import our comprehensive preprocessing pipeline\n",
    "exec(open('comprehensive_fraud_preprocessing.py').read())\n",
    "\n",
    "# Display environment info\n",
    "print(\"Environment Setup Complete!\")\n",
    "print(f\"Python Version: {sys.version}\")\n",
    "print(f\"Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Initialize the Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the fraud detection preprocessor\n",
    "data_path = \"/Users/debabratapattnayak/web-dev/learnathon/dataset\"\n",
    "output_dir = \"/Users/debabratapattnayak/web-dev/learnathon/ml_analysis_reports\"\n",
    "\n",
    "preprocessor = FraudDetectionPreprocessor(data_path, output_dir)\n",
    "print(f\"Preprocessor initialized with output directory: {preprocessor.report_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "preprocessor.load_data()\n",
    "\n",
    "print(f\"Training Data Shape: {preprocessor.combined_train.shape}\")\n",
    "print(f\"Test Data Shape: {preprocessor.test_data.shape}\")\n",
    "print(f\"\\nTraining Data Columns: {list(preprocessor.combined_train.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows of training data:\")\n",
    "display(preprocessor.combined_train.head())\n",
    "\n",
    "print(\"\\nData Info:\")\n",
    "preprocessor.combined_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform comprehensive data quality analysis\n",
    "quality_analysis = preprocessor.analyze_data_quality()\n",
    "\n",
    "print(\"=== DATA QUALITY ANALYSIS ===\")\n",
    "print(f\"\\nColumns with missing values: {len(quality_analysis['missing_values']['columns_with_missing'])}\")\n",
    "print(f\"Total duplicates: {quality_analysis['duplicates']['count']} ({quality_analysis['duplicates']['percentage']:.2f}%)\")\n",
    "\n",
    "# Show missing value percentages\n",
    "missing_pct = quality_analysis['missing_values']['percentages']\n",
    "missing_cols = [(col, pct) for col, pct in missing_pct.items() if pct > 0]\n",
    "missing_cols.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nTop columns with missing values:\")\n",
    "for col, pct in missing_cols[:10]:\n",
    "    print(f\"  {col}: {pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values using advanced strategies\n",
    "print(\"Before missing value handling:\")\n",
    "print(f\"Total missing values: {preprocessor.combined_train.isnull().sum().sum()}\")\n",
    "print(f\"Data shape: {preprocessor.combined_train.shape}\")\n",
    "\n",
    "preprocessor.handle_missing_values()\n",
    "\n",
    "print(\"\\nAfter missing value handling:\")\n",
    "print(f\"Total missing values: {preprocessor.combined_train.isnull().sum().sum()}\")\n",
    "print(f\"Data shape: {preprocessor.combined_train.shape}\")\n",
    "\n",
    "# Display handling summary\n",
    "if 'missing_values_handled' in preprocessor.preprocessing_summary:\n",
    "    mv_summary = preprocessor.preprocessing_summary['missing_values_handled']\n",
    "    print(f\"\\nStrategy used: {mv_summary['strategy_used']}\")\n",
    "    if mv_summary['high_missing_dropped']:\n",
    "        print(f\"Columns dropped: {mv_summary['high_missing_dropped']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Handle Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and handle duplicate records\n",
    "preprocessor.identify_and_handle_duplicates()\n",
    "\n",
    "if 'duplicates_handled' in preprocessor.preprocessing_summary:\n",
    "    dup_info = preprocessor.preprocessing_summary['duplicates_handled']\n",
    "    print(\"=== DUPLICATE HANDLING RESULTS ===\")\n",
    "    print(f\"Initial records: {dup_info['initial_count']}\")\n",
    "    print(f\"Final records: {dup_info['final_count']}\")\n",
    "    print(f\"Duplicates removed: {dup_info['removed_count']} ({dup_info['removal_percentage']:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Outlier Detection and Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers using multiple methods\n",
    "outlier_analysis = preprocessor.detect_outliers()\n",
    "\n",
    "print(\"=== OUTLIER ANALYSIS ===\")\n",
    "print(f\"Analyzed {len(outlier_analysis)} numerical columns\")\n",
    "\n",
    "# Show top columns with outliers\n",
    "outlier_summary = []\n",
    "for col, info in outlier_analysis.items():\n",
    "    outlier_summary.append((col, info['iqr_outliers'], info['total_values']))\n",
    "\n",
    "outlier_summary.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nTop columns with outliers (IQR method):\")\n",
    "for col, outliers, total in outlier_summary[:10]:\n",
    "    percentage = (outliers / total) * 100 if total > 0 else 0\n",
    "    print(f\"  {col}: {outliers} outliers ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers using capping method\n",
    "preprocessor.handle_outliers(method='cap')\n",
    "\n",
    "print(\"Outliers handled using capping method (1st and 99th percentiles)\")\n",
    "if 'outliers_handled' in preprocessor.preprocessing_summary:\n",
    "    outlier_info = preprocessor.preprocessing_summary['outliers_handled']\n",
    "    print(f\"\\nColumns processed: {len(outlier_info)}\")\n",
    "    \n",
    "    # Show summary for top columns\n",
    "    for col, info in list(outlier_info.items())[:5]:\n",
    "        print(f\"  {col}: {info['outliers_handled']} outliers capped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Categorical Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "print(\"Before encoding:\")\n",
    "categorical_cols = preprocessor.combined_train.select_dtypes(include=['object']).columns\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "print(f\"Columns: {list(categorical_cols)}\")\n",
    "\n",
    "preprocessor.encode_categorical_features()\n",
    "\n",
    "print(\"\\nAfter encoding:\")\n",
    "categorical_cols_after = preprocessor.combined_train.select_dtypes(include=['object']).columns\n",
    "print(f\"Remaining categorical columns: {len(categorical_cols_after)}\")\n",
    "\n",
    "if 'categorical_encoding' in preprocessor.preprocessing_summary:\n",
    "    enc_info = preprocessor.preprocessing_summary['categorical_encoding']\n",
    "    label_encoded = [col for col, info in enc_info.items() if info['method'] == 'label_encoding']\n",
    "    freq_encoded = [col for col, info in enc_info.items() if info['method'] == 'frequency_encoding']\n",
    "    \n",
    "    print(f\"\\nLabel encoded columns: {len(label_encoded)}\")\n",
    "    print(f\"Frequency encoded columns: {len(freq_encoded)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select important features using multiple methods\n",
    "selected_features = preprocessor.select_important_features(n_features=15)\n",
    "\n",
    "print(\"=== FEATURE SELECTION RESULTS ===\")\n",
    "print(f\"Total features available: {len(preprocessor.combined_train.columns) - 1}\")  # Exclude target\n",
    "print(f\"Selected features: {len(selected_features)}\")\n",
    "\n",
    "print(\"\\nTop Selected Features:\")\n",
    "for i, feature in enumerate(selected_features, 1):\n",
    "    score = preprocessor.feature_analysis['combined_scores'].get(feature, 0)\n",
    "    print(f\"{i:2d}. {feature:<25} (Score: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Feature Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize selected features\n",
    "preprocessor.normalize_features()\n",
    "\n",
    "print(\"=== FEATURE NORMALIZATION ===\")\n",
    "if 'normalization' in preprocessor.preprocessing_summary:\n",
    "    norm_info = preprocessor.preprocessing_summary['normalization']\n",
    "    print(f\"Method: {norm_info['method']}\")\n",
    "    print(f\"Features normalized: {len(norm_info['features_normalized'])}\")\n",
    "    \n",
    "    # Show before/after statistics for a few features\n",
    "    print(\"\\nNormalization Statistics (first 5 features):\")\n",
    "    for i, feature in enumerate(norm_info['features_normalized'][:5]):\n",
    "        original_mean = preprocessor.combined_train[feature].mean()\n",
    "        original_std = preprocessor.combined_train[feature].std()\n",
    "        normalized_mean = preprocessor.combined_train[f\"{feature}_normalized\"].mean()\n",
    "        normalized_std = preprocessor.combined_train[f\"{feature}_normalized\"].std()\n",
    "        \n",
    "        print(f\"  {feature}:\")\n",
    "        print(f\"    Original: mean={original_mean:.2f}, std={original_std:.2f}\")\n",
    "        print(f\"    Normalized: mean={normalized_mean:.2f}, std={normalized_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create engineered features\n",
    "engineered_features = preprocessor.create_engineered_features()\n",
    "\n",
    "print(\"=== FEATURE ENGINEERING RESULTS ===\")\n",
    "print(f\"Engineered features created: {len(engineered_features)}\")\n",
    "\n",
    "print(\"\\nNew Features:\")\n",
    "for i, feature in enumerate(engineered_features, 1):\n",
    "    print(f\"{i}. {feature}\")\n",
    "    \n",
    "    # Show basic statistics\n",
    "    if feature in preprocessor.combined_train.columns:\n",
    "        stats = preprocessor.combined_train[feature].describe()\n",
    "        print(f\"   Mean: {stats['mean']:.4f}, Std: {stats['std']:.4f}\")\n",
    "        print(f\"   Min: {stats['min']:.4f}, Max: {stats['max']:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Generate Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "preprocessor.create_preprocessing_visualizations()\n",
    "\n",
    "print(\"Visualizations created successfully!\")\n",
    "print(f\"Saved to: {preprocessor.report_dir}\")\n",
    "\n",
    "# List created visualization files\n",
    "import glob\n",
    "viz_files = glob.glob(str(preprocessor.report_dir / \"*.png\"))\n",
    "print(f\"\\nVisualization files created: {len(viz_files)}\")\n",
    "for file in viz_files:\n",
    "    print(f\"  - {os.path.basename(file)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Generate PDF Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive PDF reports\n",
    "print(\"Generating PDF reports...\")\n",
    "\n",
    "# Generate preprocessing report (PDF 1)\n",
    "pdf1_path = preprocessor.generate_preprocessing_pdf()\n",
    "print(f\"\\nPreprocessing Report (PDF 1): {pdf1_path}\")\n",
    "\n",
    "# Generate feature engineering report (PDF 2)\n",
    "pdf2_path = preprocessor.generate_feature_engineering_pdf()\n",
    "print(f\"Feature Engineering Report (PDF 2): {pdf2_path}\")\n",
    "\n",
    "print(\"\\n=== PDF REPORTS GENERATED SUCCESSFULLY ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed training data\n",
    "processed_data_path = preprocessor.report_dir / \"processed_training_data.csv\"\n",
    "preprocessor.combined_train.to_csv(processed_data_path, index=False)\n",
    "\n",
    "print(f\"Processed data saved to: {processed_data_path}\")\n",
    "print(f\"Final data shape: {preprocessor.combined_train.shape}\")\n",
    "\n",
    "# Show final column summary\n",
    "print(\"\\nFinal Dataset Summary:\")\n",
    "print(f\"Total columns: {len(preprocessor.combined_train.columns)}\")\n",
    "print(f\"Selected original features: {len(preprocessor.selected_features)}\")\n",
    "print(f\"Engineered features: {len(preprocessor.engineered_features)}\")\n",
    "print(f\"Normalized features: {len([col for col in preprocessor.combined_train.columns if '_normalized' in col])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREPROCESSING PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ“ Report Directory: {preprocessor.report_dir}\")\n",
    "print(f\"ðŸ“„ Preprocessing Report (PDF 1): preprocessing_analysis_report.pdf\")\n",
    "print(f\"ðŸ“„ Feature Engineering Report (PDF 2): feature_engineering_report.pdf\")\n",
    "print(f\"ðŸ’¾ Processed Data: processed_training_data.csv\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Data Summary:\")\n",
    "print(f\"   â€¢ Original shape: {preprocessor.preprocessing_summary['initial_train_shape']}\")\n",
    "print(f\"   â€¢ Final shape: {preprocessor.combined_train.shape}\")\n",
    "print(f\"   â€¢ Selected features: {len(preprocessor.selected_features)}\")\n",
    "print(f\"   â€¢ Engineered features: {len(preprocessor.engineered_features)}\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Ready for Model Building:\")\n",
    "print(f\"   â€¢ Data is cleaned and preprocessed\")\n",
    "print(f\"   â€¢ Features are selected and engineered\")\n",
    "print(f\"   â€¢ Normalization applied for ML algorithms\")\n",
    "print(f\"   â€¢ Comprehensive documentation generated\")\n",
    "\n",
    "print(f\"\\nðŸš€ Next Steps:\")\n",
    "print(f\"   1. Review the generated PDF reports\")\n",
    "print(f\"   2. Use processed data for model training\")\n",
    "print(f\"   3. Apply same preprocessing to test data\")\n",
    "print(f\"   4. Build and evaluate ML models\")\n",
    "print(f\"   5. Create Streamlit application\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
